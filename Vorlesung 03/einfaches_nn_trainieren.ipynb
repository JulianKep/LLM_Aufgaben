{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55f3a03-2178-448e-87a1-c59331007c4a",
   "metadata": {},
   "source": [
    "# Bigarm-Modellierung mit neuronalen Netzen\n",
    "\n",
    "Im letzten Schritt wollen wir uns nun unsere statistische Verteilung nicht direkt aus den Trainingsdaten berechnen, sondern diese in einem ganz einfachen neuronalen Netz lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a8d7a-778f-4c90-85f0-92c693241c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7495410b-64b1-4fcc-af64-9aa79b6f8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir lesen wieder unsere Daten ein - wie bei der statistischen Berechnung ebenfalls\n",
    "\n",
    "import torch\n",
    "\n",
    "lines = open(\"data/vornamenstatistik_24.csv\", \"r\", encoding=\"utf-8\").read().splitlines()\n",
    "names = list(set([n.split(\",\")[1] for n in lines][1:]))\n",
    "\n",
    "sorted_names = sorted(names, key=lambda x: len(x))\n",
    "count_chars = {}\n",
    "for w in sorted_names:\n",
    "    chars = [\"<s>\"] + list(w) + [\"<e>\"]\n",
    "    for c1 in chars:\n",
    "        count_chars[c1.lower()] = count_chars.get(c1.lower(), 0) + 1\n",
    "all_chars = list(count_chars.keys())\n",
    "sorted_chars = sorted(count_chars.items(), key=lambda x: x[1])\n",
    "frequent_chars = [i for i in all_chars if count_chars[i] >= 10]\n",
    "\n",
    "N_2 = torch.zeros((len(frequent_chars),len(frequent_chars)), dtype=torch.int32)\n",
    "\n",
    "for w in sorted_names:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for c1, c2 in zip(chars, chars[1:]):\n",
    "        if c1.lower() in frequent_chars and c2.lower() in frequent_chars:\n",
    "            id_x = frequent_chars.index(c1.lower())\n",
    "            id_y = frequent_chars.index(c2.lower())\n",
    "        # Statt in unserem Dictionary hochzuzählen, zählen wir jetzt an der Matrixposition\n",
    "        N_2[id_x, id_y] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621200f-d1eb-4b2d-902f-da64f0b76786",
   "metadata": {},
   "source": [
    "Schritt 1: Anstatt, dass wir unsere Daten jetzt in einem 2D-Tensor / einer Matrix speichern, speichern wir uns diese als Trainingsdaten ab.\n",
    "- xs ist unser Eingabedatensatz, also immer der erste Buchstabe in unserem Bigram-Paar\n",
    "- ys ist unser Ausgabedatensatz, also immer der zweite Buchstabe in unserem Bigram-Paar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a85cc0-fe43-40a8-bd3e-7e8619234543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivana\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 14, 11,  1,  2,  1]), tensor([14, 11,  1,  2,  1,  3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = []\n",
    "ys = [] \n",
    "\n",
    "for w in names[1:2]:\n",
    "    print(w)\n",
    "    chars = [\"<s>\"] + list(w) + [\"<e>\"]\n",
    "    for c1, c2 in zip(chars, chars[1:]):\n",
    "        id_x = frequent_chars.index(c1.lower())\n",
    "        id_y = frequent_chars.index(c2.lower())\n",
    "        xs.append(id_x)\n",
    "        ys.append(id_y)\n",
    "\n",
    "\n",
    "# Die Listen konvertieren wir noch zu Tensoren\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xs, ys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "910e7571-b3ab-453d-bf8d-2de6326d318b",
   "metadata": {},
   "source": [
    "Schritt 2: Um unsere Daten besser in die neuronalen Netze füttern zu können, konvertieren wir unsere Input-Daten jetzt noch in ein \"Bag-of-Words\". Dabei wird ein Buchstabe nicht mehr als Zahl / Index in einer Liste repräsentierert, sondern wir nutzen eine Vektor-Schreibweise. In diesem Vektor gibt es so viele Zeilen wie wir Buchstaben haben. Diese sind mit Nullen gefüllt. Und jetzt setzen wir nur den Index auf 1, der dem Index unseres Buchstabens entsprocht. \n",
    "Statt dass wir zum Beispiel M = 14 eingeben, geben wir einen Vektor ein, der an allen Stellen eine 0 hat, ausser an der 14. eine 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c088700-42c1-46f8-84f1-ce588a9a2f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=len(frequent_chars)).float()  # float conversion ist elementar für Input in NN\n",
    "xenc.shape\n",
    "xenc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be3e6b-ac8c-4659-a87e-c59dab39e97a",
   "metadata": {},
   "source": [
    "Wir erstellen uns noch mal einen Plot, um das ganze einfacher zu veranschaulichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d7f72f-823f-46f9-89ed-24fdc5043f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f397bf956a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83c377-d82f-411c-a263-9a808acad762",
   "metadata": {},
   "source": [
    "Schritt 3: Jetzt haben wir alles vorbereitet, um unsere Gewichtsmatrix in unserem neuronalen Netz zu trainieren. In diesem simplen Beispiel verzichten wir noch auf einige Terme aus unserem neuronalen Netz - wir nehmen simplifiziert an, dass y = x * W gilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb830d21-c0f6-4564-8f00-b81a804a6ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.0418, -0.2516,  0.8599],\n",
       "        [-1.3847, -0.8712, -0.2234,  ...,  1.8446, -1.1845,  1.3835],\n",
       "        [ 1.4451,  0.8564,  2.2181,  ..., -0.8278,  1.3347,  0.4835],\n",
       "        ...,\n",
       "        [ 0.0518, -0.3285, -2.2472,  ...,  1.4557, -0.3461, -0.2634],\n",
       "        [-0.4477, -0.7288, -0.1607,  ...,  0.5405,  0.4351, -2.2717],\n",
       "        [-0.1339, -0.0586,  0.1257,  ...,  1.1085,  0.5544,  1.5818]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wir füllen unsere Gewichte am Anfang einfach random\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W = torch.randn((len(frequent_chars), len(frequent_chars)), generator=g) \n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa981410-51ce-4e20-a965-9730cdd4597f",
   "metadata": {},
   "source": [
    "Wir haben jetzt noch nichts gelernt, wir haben nur zufällig unsere Gewichte initialisiert. Aber wir können jetzt überprüfen, wie gut unsere Initialisierung war, in dem wir uns einfach eine vorhersage von y ausrechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96dfc10c-c1a7-4393-be3a-da8396cf775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00,  6.7842e-01,\n",
       "         -1.2345e+00, -4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00,\n",
       "         -3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
       "          7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01,\n",
       "         -7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,\n",
       "          1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02,\n",
       "         -2.5158e-01,  8.5986e-01],\n",
       "        [-1.3839e+00,  4.8687e-01, -1.0020e+00,  3.2949e-02, -4.2920e-01,\n",
       "         -9.8180e-01, -6.4206e-01,  8.2659e-01,  1.5914e+00, -1.2081e-01,\n",
       "         -4.8302e-01,  1.1330e-01,  7.7151e-02, -9.2281e-01, -1.2620e+00,\n",
       "          1.0861e+00,  1.0966e+00, -6.8369e-01,  6.6043e-02, -7.7380e-04,\n",
       "          1.6206e-01,  1.1960e+00, -1.3062e+00, -1.4040e+00, -1.0597e+00,\n",
       "          3.0573e-01,  4.1506e-01, -7.1741e-01,  2.8340e+00,  1.9535e+00,\n",
       "          2.0487e+00, -1.0880e+00],\n",
       "        [-7.9394e-01,  3.7523e-01,  8.7910e-02, -1.2415e+00, -3.2025e-01,\n",
       "         -8.4438e-01, -5.5135e-01,  1.9890e+00,  1.9003e+00,  1.6951e+00,\n",
       "          2.8090e-02, -1.7537e-01, -1.7735e+00, -7.0464e-01, -3.9465e-01,\n",
       "          1.8868e+00, -2.1844e-01,  1.6630e-01,  2.1442e+00,  1.7046e+00,\n",
       "          3.4590e-01,  6.4248e-01, -2.0395e-01,  6.8537e-01, -1.3969e-01,\n",
       "         -1.1808e+00, -1.2829e+00,  4.4849e-01, -5.9074e-01,  8.5406e-01,\n",
       "         -4.9007e-01, -3.5946e-01],\n",
       "        [-1.3847e+00, -8.7124e-01, -2.2337e-01,  1.7174e+00,  3.1888e-01,\n",
       "         -4.2452e-01,  3.0572e-01, -7.7459e-01, -1.5576e+00,  9.9564e-01,\n",
       "         -8.7979e-01, -6.0114e-01, -1.2742e+00,  2.1228e+00, -1.2347e+00,\n",
       "         -4.8791e-01, -9.1382e-01, -6.5814e-01,  7.8024e-02,  5.2581e-01,\n",
       "         -4.8799e-01,  1.1914e+00, -8.1401e-01, -7.3599e-01, -1.4032e+00,\n",
       "          3.6004e-02, -6.3477e-02,  6.7561e-01, -9.7807e-02,  1.8446e+00,\n",
       "         -1.1845e+00,  1.3835e+00],\n",
       "        [ 1.4451e+00,  8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01,\n",
       "         -1.9733e-01, -1.0546e+00,  1.2780e+00, -1.7219e-01,  5.2379e-01,\n",
       "          5.6622e-02,  4.2630e-01,  5.7501e-01, -6.4172e-01, -2.2064e+00,\n",
       "         -7.5080e-01,  1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01,\n",
       "          5.3619e-01,  5.2462e-01,  1.1412e+00,  5.1644e-02,  7.4395e-01,\n",
       "         -4.8158e-01, -1.0495e+00,  6.0390e-01, -1.7223e+00, -8.2777e-01,\n",
       "          1.3347e+00,  4.8354e-01],\n",
       "        [-1.3847e+00, -8.7124e-01, -2.2337e-01,  1.7174e+00,  3.1888e-01,\n",
       "         -4.2452e-01,  3.0572e-01, -7.7459e-01, -1.5576e+00,  9.9564e-01,\n",
       "         -8.7979e-01, -6.0114e-01, -1.2742e+00,  2.1228e+00, -1.2347e+00,\n",
       "         -4.8791e-01, -9.1382e-01, -6.5814e-01,  7.8024e-02,  5.2581e-01,\n",
       "         -4.8799e-01,  1.1914e+00, -8.1401e-01, -7.3599e-01, -1.4032e+00,\n",
       "          3.6004e-02, -6.3477e-02,  6.7561e-01, -9.7807e-02,  1.8446e+00,\n",
       "         -1.1845e+00,  1.3835e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xenc @ W\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea0c48-3118-4ee3-afd7-c0b42820b1a7",
   "metadata": {},
   "source": [
    "Schritt 4: Wir würden den Output jetzt gern wieder in etwas umwandeln, das leichter zu interpretieren ist - Eine Wahrscheinlichkeitsverteilung. \n",
    "Dafür können wir Softmax verwenden, damit können wir jeden Output in eine Wahrscheinlichkeitsverteilung transferieren.\n",
    "\n",
    "Softmax: https://de.wikipedia.org/wiki/Softmax-Funktion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f2a712-0311-4264-961b-c9f92f5a2a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1063, 0.0685, 0.0381, 0.0019, 0.0305, 0.0045, 0.0148, 0.0031, 0.0073,\n",
       "         0.0805, 0.0105, 0.0038, 0.0075, 0.0088, 0.0072, 0.0332, 0.0800, 0.0132,\n",
       "         0.0094, 0.0240, 0.0073, 0.0455, 0.0345, 0.0831, 0.0556, 0.0566, 0.0285,\n",
       "         0.0588, 0.0123, 0.0161, 0.0120, 0.0366],\n",
       "        [0.0038, 0.0247, 0.0056, 0.0157, 0.0099, 0.0057, 0.0080, 0.0347, 0.0746,\n",
       "         0.0135, 0.0094, 0.0170, 0.0164, 0.0060, 0.0043, 0.0450, 0.0455, 0.0077,\n",
       "         0.0162, 0.0152, 0.0179, 0.0503, 0.0041, 0.0037, 0.0053, 0.0206, 0.0230,\n",
       "         0.0074, 0.2585, 0.1072, 0.1179, 0.0051],\n",
       "        [0.0071, 0.0229, 0.0172, 0.0045, 0.0114, 0.0068, 0.0091, 0.1148, 0.1051,\n",
       "         0.0856, 0.0162, 0.0132, 0.0027, 0.0078, 0.0106, 0.1037, 0.0126, 0.0186,\n",
       "         0.1341, 0.0864, 0.0222, 0.0299, 0.0128, 0.0312, 0.0137, 0.0048, 0.0044,\n",
       "         0.0246, 0.0087, 0.0369, 0.0096, 0.0110],\n",
       "        [0.0052, 0.0086, 0.0165, 0.1148, 0.0283, 0.0135, 0.0280, 0.0095, 0.0043,\n",
       "         0.0558, 0.0085, 0.0113, 0.0058, 0.1721, 0.0060, 0.0126, 0.0083, 0.0107,\n",
       "         0.0223, 0.0349, 0.0126, 0.0678, 0.0091, 0.0099, 0.0051, 0.0214, 0.0193,\n",
       "         0.0405, 0.0187, 0.1303, 0.0063, 0.0822],\n",
       "        [0.0805, 0.0447, 0.1743, 0.0320, 0.0268, 0.0156, 0.0066, 0.0681, 0.0160,\n",
       "         0.0320, 0.0201, 0.0291, 0.0337, 0.0100, 0.0021, 0.0090, 0.0192, 0.0135,\n",
       "         0.0050, 0.0106, 0.0324, 0.0321, 0.0594, 0.0200, 0.0399, 0.0117, 0.0066,\n",
       "         0.0347, 0.0034, 0.0083, 0.0721, 0.0308],\n",
       "        [0.0052, 0.0086, 0.0165, 0.1148, 0.0283, 0.0135, 0.0280, 0.0095, 0.0043,\n",
       "         0.0558, 0.0085, 0.0113, 0.0058, 0.1721, 0.0060, 0.0126, 0.0083, 0.0107,\n",
       "         0.0223, 0.0349, 0.0126, 0.0678, 0.0091, 0.0099, 0.0051, 0.0214, 0.0193,\n",
       "         0.0405, 0.0187, 0.1303, 0.0063, 0.0822]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = y_pred.exp() /y_pred.exp().sum(1, keepdims = True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9608e-a063-469b-924e-8a6cb7c73173",
   "metadata": {},
   "source": [
    "Wir können jetzt für verschiedene W-initialisierungen ausprobieren, wie unterschiedlich der Output ist.\n",
    "Intuitiv wollen wir jetzt also W solange verändern, dass der Output für unsere realen Beispiele (unsere Trainingsdaten) möglichst hoch ist. Das machen wir in der Praxis natürlich nicht, in dem wir einfach so lange zufällige Verteilungen testen, bis wir eine gute gefunden haben. Stattdessen wollen wir jetzt W langsam so anpassen, dass die Vorhersagen immer besser werden. Oder in anderen Worten: Das unser Loss immer kleiner wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a96bdf8-a8ba-4baa-afc1-b276cdfeec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9718)\n",
      "tensor(3.9718)\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "ys = [] \n",
    "\n",
    "for w in names:\n",
    "    chars = [\"<s>\"] + list(w) + [\"<e>\"]\n",
    "    valid = True\n",
    "    for c in list(w): # Den Teil müssen wir einbauen, um nur Kombinationen mit unseren häufigen Buchstaben zuzulassen\n",
    "        if c.lower() not in frequent_chars:\n",
    "            valid = False\n",
    "    if valid:\n",
    "        for c1, c2 in zip(chars, chars[1:]):\n",
    "            id_x = frequent_chars.index(c1.lower())\n",
    "            id_y = frequent_chars.index(c2.lower())\n",
    "            xs.append(id_x)\n",
    "            ys.append(id_y)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=len(frequent_chars)).float() \n",
    "res = xenc @ W\n",
    "probs = res.exp() / res.exp().sum(1, keepdims = True)\n",
    "\n",
    "sum_log = 0\n",
    "for c in range(len(xs)):\n",
    "    x = xs[c].item()\n",
    "    y = ys[c].item()\n",
    "    p = probs[c, y]\n",
    "    logp = torch.log(p)\n",
    "    nll = -logp\n",
    "    sum_log += nll\n",
    "print(sum_log / len(xs))\n",
    "\n",
    "# Hier noch ein etwas hüberer Weg das ganze zu schreiben\n",
    "loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ec2cf-9e1a-446b-a636-373467188878",
   "metadata": {},
   "source": [
    "Wenig überraschend haben wir hier ein höheren Loss als wir ihn in unserer intuitiven statistischen Modellierung haben. Aber wir haben das Netz ja auch noch nicht trainiert!\n",
    "Was wir bisher gemacht haben nennt man einen Foward-Pass: Wir haben Trainingsdaten in unser Netz gesteckt und uns ausgerechnet, wie gut das momentan \"trainierte\" Netz funktioniert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0ddbd-dcc9-42ca-a553-9815759fd116",
   "metadata": {},
   "source": [
    "Schritt 5: Jetzt trainieren wir unser Netz tatsächlich. Dafür fügen wir unserem foward-pass jetzt einen back-ward pass hinzu: Wir lassen Torch den Gradienten berechnen und korrigieren auf Grund dessen unsere Gewichtsmatrix entsprechend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5eb9e09-ce3c-4d53-8a00-b938b8d532b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "W = torch.randn((len(frequent_chars), len(frequent_chars)), generator=g, requires_grad=True) # Nicht vergessen hier requires_grad auf True zu setzen, damit python weiß, dass wir hier später Gradienten für berechnen wollen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3e7d778-014f-4951-a8b8-e5a34275a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9718308448791504\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=len(frequent_chars)).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1, keepdims=True) \n",
    "loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42ee74d4-3a9f-4cca-90ea-78f418632ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5558e-02, -8.6826e-03, -1.1017e-03,  ...,  2.0050e-03,\n",
       "          1.7614e-03,  5.3523e-03],\n",
       "        [ 7.4337e-04,  3.2472e-04, -2.1021e-02,  ...,  1.8677e-02,\n",
       "         -1.2327e-03,  1.1842e-02],\n",
       "        [ 5.8536e-03, -1.1074e-02,  7.3277e-03,  ...,  6.0299e-04,\n",
       "          4.7319e-03,  2.2377e-03],\n",
       "        ...,\n",
       "        [ 1.9740e-05, -3.7476e-05,  1.9810e-06,  ...,  8.0364e-05,\n",
       "          1.3260e-05,  1.4403e-05],\n",
       "        [ 7.1615e-05, -3.0273e-04, -2.1040e-04,  ...,  1.9238e-04,\n",
       "          1.7314e-04,  1.1557e-05],\n",
       "        [ 1.0567e-05,  1.1393e-05,  1.3699e-05,  ...,  3.6600e-05,\n",
       "          2.1031e-05,  5.8752e-05]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward pass\n",
    "W.grad = None\n",
    "loss.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0735c4b4-59ca-4a4c-b520-64285b2159c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57121c45-dbb8-4b80-bc25-3cf846a074a9",
   "metadata": {},
   "source": [
    "Wenn wir jetzt die Gewichte updaten und das immer wieder machen, dann sehen wir, dass der loss langsam nach unten geht. Wir haben etwas gelernt - yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a45a6d5f-6342-4433-b4de-69b312ba0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.484388828277588\n"
     ]
    }
   ],
   "source": [
    "for i in range(500): # Hier kann geändert werden, wie viele Trainingsdurchläufe wir durchführen\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=len(frequent_chars)).float() \n",
    "    logits = xenc @ W \n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "    # print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None \n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -10 * W.grad # Der Parameter hier ist die Learning Rate\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2eb967-bb43-4524-89e1-80b5f83375d8",
   "metadata": {},
   "source": [
    "Wir sehen, dass der Loss über die Zeit beständig fällt. Und wenn wir das Training sehr oft durchführen, dann erhalten wir ungefähr den gleichen Loss, wie wir in unserem Originaldatensatz auch vorliegen hatten.\n",
    "Mehr kann das Netz nun nicht mehr lernen: Es hat aus den Trainingsdaten das Optimum herausgeholt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a336971-c1ed-4f28-9be0-19963b94bdc1",
   "metadata": {},
   "source": [
    "Schritt 6: Als letztes nutzen wir noch unser trainiertes Netzwerk, und lassen uns daraus neue Namen generieren. \n",
    "Weil wir ja kein wirkliches Netz trainiert haben, müssen wir auch hier wieder ein bisschen mogeln und einfach wieder mit der trainierten Wahrscheinlichkeitsverteilung die nächste Auswahl treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8e3de98-6823-4101-8b99-53657ec67e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adria', 'f', 'maná-ka', 'dka', 'yanaréla', 'e', 'la', 'gada', 'l', 'cha']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "generated_names = []\n",
    "for i in range(0, 10):\n",
    "    curr_char = \"<s>\"\n",
    "    name = \"\"\n",
    "    while curr_char != \"<e>\":\n",
    "        xenc = F.one_hot(torch.tensor([frequent_chars.index(curr_char)]), num_classes=len(frequent_chars)).float()\n",
    "        res = xenc @ W \n",
    "        p = res.exp() / res.exp().sum(1, keepdims=True)\n",
    "        \n",
    "        ind = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        char = frequent_chars[ind]\n",
    "        curr_char = char\n",
    "        name += curr_char\n",
    "    generated_names.append(name[:-3])\n",
    "generated_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e342db-8047-4956-a4e5-504226d9559d",
   "metadata": {},
   "source": [
    "Auch hier sehen wir wiede die gleichen Probleme wie bei unserer simplen statistischen Berechenung. Aber das war zu erwaten - wir haben ja bisher nur ein sehr einfaches, lineares Netz trainiert, und ihm auch nur einen sehr kleinen Kontext zum lernen zur Verfügung gestellt.\n",
    "Trotzdem haben wir jetzt unser erstes eigenes Mini-Language-Model erschaffen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dae754-ad2c-4e27-a390-5ccfaa05c27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
